---
title: "Build the MTPL1 Frequency Model"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    fig_caption: yes
    toc_depth: 3
    use_bookdown: yes

  html_document:
    fig_caption: yes
    theme: spacelab
    highlight: pygments
    number_sections: TRUE
    toc: TRUE
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE

  pdf_document: default
---


```{r import_libraries, echo=FALSE, message=FALSE}
library(conflicted)
library(tidyverse)
library(scales)
library(cowplot)
library(magrittr)
library(rlang)
library(glue)
library(purrr)
library(furrr)
library(fs)
library(rsample)
library(rstan)
library(rstanarm)
library(posterior)
library(bayesplot)
library(tidybayes)
library(DT)


source("custom_functions.R")

resolve_conflicts(
  c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2", "rsample")
  )


knitr::opts_chunk$set(
  tidy       = FALSE,
  cache      = FALSE,
  warning    = FALSE,
  message    = FALSE,
  fig.height =     8,
  fig.width  =    11
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallel::detectCores()
  )

theme_set(theme_cowplot())

rstan_options(auto_write = TRUE)

set.seed(42)
stan_seed <- 4242
```

In this workbook we switch our attention to building a frequency model for the
claims data. We build a number of different models and compare them in terms of
both accuracy (estimate of the mean), and precision (estimate of the
variance/dispersion).

All of our modelling is done within a Bayesian context, so rather than
estimating a single set of parameters for our model we instead estimate the
posterior distribution of the joint distribution of the parameters given the
observed data.

We use prior predictive checks to set our priors and then use Monte Carlo
simulation and posterior predictive checks to assess the quality of the various
models.


# Data and Setup

Before we do any modelling we need to load our data. Data exploration and
various data cleaning etc has been performed in a previous workbook, so we
simply load the data as-is.

It may be necessary to perform some simple feature engineering, but this is
part of the modelling process so we will instead do that here as it is an
intrinsic part of the modelling process in most cases.

```{r load_mtpl1_dataset, echo=TRUE}
modelling1_data_tbl <- read_rds("data/modelling1_data_tbl.rds")

modelling1_data_tbl %>% glimpse()
```

This dataset will be the basis for all our subsequent work with the MTPL1 data.

For the purposes of effective model validation we need to construct a
"hold out" or *testing* set. We subset the data now and do not investigate or
check this data till we have final models we wish to work with.

The size of this hold-out set is a matter of discussion, and is a trade-off
between ensuring enough data for modelling, but also ensuring the test set is
large enough to test the final models.

We sample this data at random for now, and take hold out 20% of it.

```{r construct_mtpl1_train_holdout, echo=TRUE}
mtpl1_split <- modelling1_data_tbl %>% initial_split(prop = 0.8)

mtpl1_training_tbl <- mtpl1_split %>% training()
mtpl1_training_tbl %>% glimpse()

mtpl1_testing_tbl  <- mtpl1_split %>% testing()
mtpl1_testing_tbl %>% glimpse()
```



# Constructing the Frequency Model

We start with a simple frequency model for the car insurance data, using prior
predictive checks to the set our prior parameters. The first time we do this
we will discuss this in more detail to explain the method and what we are
trying to achieve. Once we are happy with our prior model we then switch to
conditioning it on the data - check the *posterior shrinkage* and estimation
of how informative our data has been on the model, and then use the output to
guide our work.


## Constructing Our Prior Model

We start by building a simple model with a small number of parameters. Going
by our previous data exploration, we use `gas` and `cat_driver_age`. Later
models will use a smoothed predictor for our continuous variables where there
is a nonlinear effect, but for now we focus on the discretisations of those
variables for simplicity.

In formula notation, our model will look something like this:

```
claim_count ~ gas + cat_driver_age
```

Since `claim_count` is a count variable we will use some form of count
regression: either Poisson or Negative Binomial, and will will try both.

Our idea is to have our parameters vary on a unit scale, and so Normal priors
should be fine. This leaves the intercept, so we start with a Normal prior here
also and see what the effect is.

For the purposes of creating some summary information on the mean frequency for
our frequency model, we create a simple summary function that calculates this
summary information in a standard way.

```{r calculate_mean_freq_summary, echo=TRUE}
calculate_mean_freq_summary <- function(data_tbl) {
  summary_tbl <- data_tbl %>%
    summarise(
      .groups = "drop",
  
      freqmean_min  = min(freq_mean) %>% round(4),
      freqmean_p10  = quantile(freq_mean, 0.10) %>% round(4),
      freqmean_p25  = quantile(freq_mean, 0.25) %>% round(4),
      freqmean_p50  = quantile(freq_mean, 0.50) %>% round(4),
      freqmean_p75  = quantile(freq_mean, 0.75) %>% round(4),
      freqmean_p90  = quantile(freq_mean, 0.90) %>% round(4),
      freqmean_max  = max(freq_mean) %>% round(4),

      freqmean_mean = mean(freq_mean) %>% round(4),
      freqmean_sd   = sd(freq_mean) %>% round(4)
      )

  return(summary_tbl)
}


```


### Our First Prior Model

We use the `rstanarm` package to fit this model - this allow us to use standard
R model notation and formula in a Bayesian context, and avoids us the tedious
of work of having to write out the full Stan code for this problem.

To fit from the prior predictive rather than conditioning on the data, the
model will not add the observed data and simply fit from the priors.

```{r fit_first_prior_model, echo=TRUE}
fit_data_tbl <- mtpl1_training_tbl %>% select(-sev_data)

mtpl1_freq1_prior_stanreg <- stan_glm(
    claim_count ~ gas + cat_driver_age,
    family   = poisson(),
    data     = fit_data_tbl,
    offset   = log(exposure),
    iter     = 500,
    chains   = 4,
    QR       = TRUE,
    prior    = normal(location = 0, scale = 1),
    prior_PD = TRUE,
    seed     = stan_seed
    )

mtpl1_freq1_prior_stanreg %>% summary()
```

We do not need to look at the whole dataset for the purposes of calibrating
our priors, so we take a subsample of that data to help us build our priors
using the `generated quantities` feature in Stan to generate a sample count of
policies for each of the iterations in the posterior sample.

```{r construct_prior_predictive_data, echo=TRUE}
n_sample <- 250

priorpred_input_tbl <- fit_data_tbl %>%
  slice_sample(n = n_sample) %>%
  arrange(policy_id)

priorpred_input_tbl %>% glimpse()
```

We now use this data to generate simulations of counts.

```{r construct_prior_predictive_sample, echo=TRUE}
priorpred_freq1_tbl <- priorpred_input_tbl %>%
  add_linpred_draws(
    model  = mtpl1_freq1_prior_stanreg,
    offset = rep(1, n_sample),
    value  = "freq_mean"
    ) %>%
  ungroup()

priorpred_freq1_tbl %>% glimpse()
```

We then produce some summary statistics of these count frequencies, comparing
those to our domain knowledge of what is possible in the real world. We use
this to tweak our priors in a principled way.

```{r freq1_calculate_prior_predict_summaries, echo=TRUE}
priorpred_freq1_summary_tbl <- priorpred_freq1_tbl %>%
  group_by(.row) %>%
  calculate_mean_freq_summary()

priorpred_freq1_summary_tbl %>% glimpse()
```

We will output this as a table to HTML to allow for proper inspection.

```{r plot_table_as_dt, echo=TRUE}
priorpred_freq1_summary_tbl %>% datatable(rownames = FALSE)
```

We see we need to make changes to these priors as we end up with claim counts
that are massively in excess of any feasible number. We expect inputs to the
model to have both positive and negative effects on the claim count, and so
we focus on the prior for the intercept - shifting the mean much lower.



### A Second Prior Model

Our second prior model is similar to the first one, but with the prior on the
intercept set at a mean of -4 - thus reducing the 'baseline' claim rate.

```{r fit_second_prior_model, echo=TRUE}
mtpl1_freq1_prior2_stanreg <- stan_glm(
    claim_count ~ gas + cat_driver_age,
    family   = poisson(),
    data     = fit_data_tbl,
    offset   = log(exposure),
    iter     = 500,
    chains   = 4,
    QR       = TRUE,
    prior_PD = TRUE,
    seed     = stan_seed,
    prior_intercept = normal(location = -4, scale = 1),
    prior           = normal(location =  0, scale = 1)
    )

priorpred_freq2_summary_tbl <- priorpred_input_tbl %>%
  add_linpred_draws(
    model  = mtpl1_freq1_prior2_stanreg,
    offset = rep(1, n_sample),
    value  = "freq_mean"
    ) %>%
  ungroup() %>%
  group_by(.row) %>%
  calculate_mean_freq_summary()

priorpred_freq2_summary_tbl %>% glimpse()
```

We look at these summaries as before.

```{r plot_new_table_as_dt, echo=TRUE}
priorpred_freq2_summary_tbl %>% datatable(rownames = FALSE)
```

This looks much better - though the values of `freqmean_max` may be a little
small, so we produce a histogram of those values as a check.

```{r plot_priorpred_freqmean_max_histogram, echo=TRUE}
ggplot(priorpred_freq2_summary_tbl) +
  geom_histogram(aes(x = freqmean_max), binwidth = 1) +
  xlab("Maximum Number of Claims") +
  ylab("Count Frequency") +
  ggtitle("Histogram of Maximum Counts of Claims")
```

As suspected, these numbers as a bit of the low side - individual policies
could have claim counts in the double digits. High risk policies sometimes
have 20 or even 30 claims on them in a given year. These counts are unlikely
but certainly possible, so our priors should allow for this.


### Add Prior Autoscaling

We also need to investigate the use of the `autoscale` option in setting our
priors, which scales the parameters of the prior in line with the data. We
are unsure of the effect of this autoscaling so it is worth some investigation.


```{r fit_third_prior_model, echo=TRUE}
mtpl1_freq1_prior3_stanreg <- stan_glm(
    claim_count ~ gas + cat_driver_age,
    family   = poisson(),
    data     = fit_data_tbl,
    offset   = log(exposure),
    iter     = 500,
    chains   = 4,
    QR       = TRUE,
    prior_PD = TRUE,
    seed     = stan_seed,
    prior_intercept = normal(location = -4, scale = 1, autoscale = TRUE),
    prior           = normal(location =  0, scale = 1, autoscale = TRUE)
    )

priorpred_freq3_summary_tbl <- priorpred_input_tbl %>%
  add_linpred_draws(
    model  = mtpl1_freq1_prior3_stanreg,
    offset = rep(1, n_sample),
    value  = "freq_mean"
    ) %>%
  ungroup() %>%
  group_by(.row) %>%
  calculate_mean_freq_summary()

priorpred_freq3_summary_tbl %>% glimpse()
```

The values with autoscaling seem ludicrous, so it is worth doing a side-by-side
comparison of the values to see how the two sets of priors compare.

```{r compare_nonscaled_autoscaled_prior_params, echo=TRUE}
compare_params_tbl <- list(
    no_scale   = mtpl1_freq1_prior2_stanreg %>% tidy_draws(),
    auto_scale = mtpl1_freq1_prior3_stanreg %>% tidy_draws()
    ) %>%
  bind_rows(.id = "prior_model") %>%
  pivot_longer(
    !c(prior_model, .chain, .iteration, .draw),
    names_to  = "parameter",
    values_to = "value"
    )

plot_tbl <- compare_params_tbl %>%
  filter(str_detect(parameter, "__$", negate = TRUE))

ggplot(plot_tbl) +
  geom_boxplot(aes(x = parameter, y = value, colour = prior_model), position = "dodge") +
  labs(x = "Parameter", y = "Value", colour = "Scaling") +
  ggtitle("Auto-Scaling Comparison of Prior Parameters") +
  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5))
```

Autoscaling seems to cause our regression parameters to be much too large, so
we will probably not spend too much time with autoscaling enabled for the
parameters, but it does raise questions once we add data to these models, so
we will not ignore them completely for now, and will probably return to them
a little later once we start conditioning our models with data.

For the moment though, while we will include them in the iterative search, we
will exclude them from the data visualisations.
























# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
sessioninfo::session_info()
```
